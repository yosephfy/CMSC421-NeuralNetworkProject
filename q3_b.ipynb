{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3_B - General Deep Learning But with Multi-dimensional data\n",
    "\n",
    "The goal of this assignment is to implement a fully connected neural network with a single hidden layer and a ReLU (Rectified Linear Unit) activation function. The network should be flexible enough to accommodate any number of units in the hidden layer and any size of input, while having just one output unit.\n",
    "\n",
    "## 1.1 Network Architecture\n",
    "\n",
    "- The simple linear regression can be visualized as a neural network:\n",
    "  - **Input Layer**: It accepts the input feature.\n",
    "  - **Hidden Layer**: Contains one unit which simply passes the value.\n",
    "  - **Bias Layer**: An added constant to introduce flexibility to the model.\n",
    "  - **ReLU Activation**: Introduces non-linearity into the model. This means that the model using ReLU can be trained to approximate complex, non-linear functions, which is essential for learning from the data.\n",
    "  - **Output Layer**: Produces the prediction for the given input.\n",
    "\n",
    "Mathematically, the output of the network is represented as:\n",
    "<!-- $$ a_1 = Xw_0 + a_0 + b_1 $$ -->\n",
    "\n",
    "$$ \\ a_2 = \\sum_{k=1}^{d} Xw_{2k} a_{1k} + b_2 \\ $$\n",
    "$$ OR $$\n",
    "$$ \\hat{y} = g(\\mathcal{H^{l}} \\mathcal{W^{(l+1)}} + \\mathcal{b^{(l+1)}}) $$\n",
    "\n",
    "\n",
    "<!-- $$ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - a_1(x_i))^2 $$ -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ######\n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "# This Cell Imports all the required Libraries.\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import collections.abc\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Data Utility Functions\n",
    "\n",
    "This section provides utility functions to generate and manipulate data, mainly focusing on data splitting and data generation.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. `train_test_split` Function:\n",
    "\n",
    "This function is responsible for splitting the given data into training and testing sets based on the specified `test_size` fraction.\n",
    "\n",
    "- **Input**:\n",
    "  - `x`: Feature data.\n",
    "  - `y`: Target labels.\n",
    "  - `test_size`: Fraction of the dataset to be used as testing data (default is 0.2, i.e., 20%).\n",
    "\n",
    "- **Output**:\n",
    "  - Training and testing data for both features and labels.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. `generate_sine_data` Function:\n",
    "\n",
    "Generates data based on the sine function. This function simulates a non-linear relationship.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `num_samples`: Number of data points to generate.\n",
    "  - `dimension`: Dimension of the input data.\n",
    "  - `amplitude`, `frequency`, `phase`: Parameters of the sine function.\n",
    "  - `categorical`: If set to `True`, the output is binarized based on the median value.\n",
    "  \n",
    "- **Output**:\n",
    "  - Training and testing data for both features and labels.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. `generate_data` Function:\n",
    "\n",
    "Generates linear data with optional noise. This function simulates a linear relationship.\n",
    "\n",
    "- **Parameters**:\n",
    "  - `num_samples`: Number of data points to generate.\n",
    "  - `dimension`: Dimension of the input data.\n",
    "  - `m` and `b`: Slope and intercept of the linear relationship.\n",
    "  - `categorical`: If set to `True`, the output is binarized based on the median value.\n",
    "  \n",
    "- **Output**:\n",
    "  - Training and testing data for both features and labels.\n",
    "\n",
    "---\n",
    "\n",
    "#### Utility Functions:\n",
    "\n",
    "- `q2_a` and `q2_b`: \n",
    "  - These functions are wrappers around the data generating functions. They ensure that data is correctly formatted and split before being used in the subsequent processing.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: The main intent behind these utility functions is to ease the data generation and preparation steps. You do not need to edit these functions for most tasks. They're here to ensure you have the necessary data structure for your experiments.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ######\n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "\n",
    "def train_test_split(x, y, test_size=0.2):\n",
    "\n",
    "    # Split data\n",
    "    num_samples = len(x)\n",
    "    num_test = int(test_size * num_samples)\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    test_indices = indices[:num_test]\n",
    "    train_indices = indices[num_test:]\n",
    "\n",
    "    x_train = [x[i] for i in train_indices]\n",
    "    x_test = [x[i] for i in test_indices]\n",
    "\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def generate_sine_data(num_samples=100, dimension=1, test_size=0.2, amplitude=1, frequency=1, phase=0, low=0, high=10, categorical=False):\n",
    "    x = np.random.uniform(low=low, high=high, size=(num_samples, dimension))\n",
    "\n",
    "    if dimension == 1:\n",
    "        y = amplitude * np.sin(frequency * x + phase)\n",
    "    else:\n",
    "        y = np.zeros((num_samples, 1))\n",
    "        for d in range(dimension):\n",
    "            y += amplitude * np.sin(frequency * x[:, d].reshape(-1, 1) + phase)\n",
    "\n",
    "    if categorical:\n",
    "        median = np.median(y)\n",
    "        labels = np.zeros(num_samples, dtype=int)\n",
    "        labels[y[:, 0] <= median] = 0\n",
    "        labels[y[:, 0] > median] = 1\n",
    "        y = labels\n",
    "        y = y.astype(int)\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size)\n",
    "\n",
    "    return np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)\n",
    "\n",
    "def generate_data(num_samples=100, dimension=1, test_size=0.2, m=7, b=3, low=0, high=10, categorical=False):\n",
    "\n",
    "    # Generate x values\n",
    "    x = np.random.uniform(low=low, high=high, size=(num_samples, dimension))\n",
    "\n",
    "    # Compute y values with noise\n",
    "    if dimension == 1:\n",
    "        # noise = np.random.normal(loc=0, scale=1, size=num_samples)\n",
    "        noise = np.random.normal(loc=0, scale=1, size=(num_samples, 1))\n",
    "        y = (m * x) + b + noise\n",
    "\n",
    "    else:\n",
    "        noise = np.random.normal(loc=0, scale=1, size=num_samples)\n",
    "        y = np.dot(x, np.array([m] * dimension)) + b + noise\n",
    "\n",
    "    y = y.reshape(-1, 1)  # Make y a column vector\n",
    "\n",
    "    if categorical:\n",
    "        median = np.median(y)\n",
    "        labels = np.zeros(num_samples, dtype=int)\n",
    "        labels[y[:, 0] <= median] = 0\n",
    "        labels[y[:, 0] > median] = 1\n",
    "        print(median)\n",
    "        y = labels\n",
    "        y = y.astype(int)\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "    # Split data into train/test\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size)\n",
    "\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "def q2_a():\n",
    "    xtrain, xtest, ytrain, ytest = generate_sine_data(num_samples=10000, dimension=1, test_size=0.2)\n",
    "    xtrain, xtest, ytrain, ytest = np.array(xtrain, dtype=np.float32), np.array(xtest, dtype=np.float32), np.array(ytrain, dtype=np.float32), np.array(ytest, dtype=np.float32)\n",
    "    print(xtrain.shape, xtest.shape, ytrain.shape, ytest.shape)\n",
    "    print(xtrain)\n",
    "    print(ytrain)\n",
    "    return {\n",
    "        \"train\": (xtrain, ytrain),\n",
    "        \"test\": (xtest, ytest)\n",
    "    }\n",
    "\n",
    "\n",
    "def q2_b():\n",
    "    xtrain, xtest, ytrain, ytest = generate_sine_data(num_samples=100000, dimension=5, test_size=0.2, low=0, high=10)\n",
    "    xtrain, xtest, ytrain, ytest = np.array(xtrain, dtype=np.float32), np.array(xtest, dtype=np.float32), np.array(ytrain, dtype=np.float32), np.array(ytest, dtype=np.float32)\n",
    "    print(xtrain.shape, xtest.shape, ytrain.shape, ytest.shape)\n",
    "    print(xtrain)\n",
    "    print(ytrain)\n",
    "    return {\n",
    "        \"train\": (xtrain, ytrain),\n",
    "        \"test\": (xtest, ytest)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `LinearLayer` Class: Fully Connected Layer\n",
    "\n",
    "This class represents a fully connected (or linear) layer within a neural network. This is the foundation of dense layers in many deep learning models, where every neuron in the current layer is connected to every neuron in the next layer.\n",
    "\n",
    "---\n",
    "\n",
    "#### Attributes:\n",
    "\n",
    "- **`input_layer`**: The preceding layer in the neural network.\n",
    "- **`output_dimension`**: Specifies the number of neurons in the current layer.\n",
    "- **`W`**: Weights matrix that will be learned during training.\n",
    "\n",
    "---\n",
    "\n",
    "#### Methods:\n",
    "\n",
    "1. **`__init__(self, input_layer, number_out_features)`**: \n",
    "    - Initializes the layer based on the dimensions of the input layer and the desired number of output features.\n",
    "    - Ensures that the input layer contains a list of 1D linear feature data.\n",
    "    - Initializes weights randomly.\n",
    "\n",
    "2. **`forward(self)`**:\n",
    "    - Computes the forward pass for the layer, essentially multiplying the input data by the weight matrix (`XW`).\n",
    "\n",
    "3. **`backward(self, downstream)`**:\n",
    "    - Computes the backward pass, propagating the gradient backward to the input layer and adjusting weights based on the propagated gradient.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: Care has been taken to ensure that the weights are not initialized with NaN or Infinity values, and this is checked again during the forward pass. It's essential to ensure that these do not propagate through the neural network, as they would lead to unstable training dynamics.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ######\n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "\n",
    "class LinearLayer:\n",
    "    \"\"\"\n",
    "    A class representing a fully connected (linear) layer in a neural network.\n",
    "\n",
    "    Methods:\n",
    "        forward(): Computes the forward pass of the layer.\n",
    "        backward(dwnstrm): Computes the backward pass, propagating the gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_layer, number_out_features) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the layer.\n",
    "\n",
    "        Parameters:\n",
    "            input_layer: The preceding layer in the neural network.\n",
    "            output_dimension: Number of neurons in the current layer.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If input layer dimensions are not a list of 1D linear feature data.\n",
    "        \"\"\"\n",
    "        assert len(input_layer.output_dimension) == 2, \"Input layer must contain a list of 1D linear feature data.\"\n",
    "        self.input_layer = input_layer\n",
    "        num_data, num_in_features = input_layer.output_dimension\n",
    "        self.output_dimension = np.array([num_data, number_out_features])\n",
    "        assert num_in_features > 0, \"num_in_features should be greater than 0\"\n",
    "        np.random.seed(42)\n",
    "\n",
    "        self.W = np.random.randn(num_in_features, number_out_features) / np.sqrt(num_in_features)\n",
    "        assert not np.isnan(self.W).any(), \"Initial weights contain NaN\"\n",
    "        assert not np.isinf(self.W).any(), \"Initial weights contain Inf\"\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Compute the forward pass for the layer, i.e., compute XW.\n",
    "        \"\"\"\n",
    "        self.input_array = self.input_layer.forward()\n",
    "        assert not np.isnan(self.input_array).any(), \"Input array contains NaN\"\n",
    "        assert not np.isnan(self.W).any(), \"Weights contain NaN before update\"\n",
    "        assert not np.isinf(self.W).any(), \"Weights contain Inf before update\"\n",
    "\n",
    "        self.output_array = self.input_array @ self.W\n",
    "        return self.output_array\n",
    "\n",
    "    def backward(self, downstream):\n",
    "        \"\"\"\n",
    "        Compute the backward pass for the layer, propagating the gradient backward.\n",
    "        \"\"\"\n",
    "        self.G = self.input_array[:, :, np.newaxis] * downstream[:, np.newaxis]\n",
    "\n",
    "        input_grad = (self.W @ downstream[:, :, np.newaxis]).squeeze(axis=-1)\n",
    "        self.input_layer.backward(input_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `HiddenLayer` Class\n",
    "\n",
    "`HiddenLayer` is a class that models a hidden layer in a neural network. Building on the foundation of the `LinearLayer` class, it introduces activation functionality to it. As the name suggests, this layer is generally used in the middle layers of deep neural networks to enable non-linear transformations of the input data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Methods:\n",
    "\n",
    "1. **Constructor: `__init__(self, input_dimension, output_dimension)`**\n",
    "    - **Purpose**: Initializes the HiddenLayer.\n",
    "    - **Parameters**:\n",
    "        - `input_dimension`: Specifies the number of input features.\n",
    "        - `output_dimension`: Dictates the number of output features or neurons in the hidden layer.\n",
    "    - **Behavior**: Calls the constructor of the parent `LinearLayer` class to handle weight initialization and set up dimensions.\n",
    "\n",
    "2. **`forward(self)`**:\n",
    "    - **Purpose**: Handles the forward propagation of data in the neural network.\n",
    "    - **Behavior**: Takes input data and conducts a linear transformation. In this implementation, an activation function's operation is implied but not explicitly included. For real-world cases, an activation like ReLU, Sigmoid, etc., would be applied to the output of this function.\n",
    "    - **Returns**: \n",
    "        - `_out`: The linearly transformed data.\n",
    "\n",
    "3. **`backward(self, downstream)`**:\n",
    "    - **Purpose**: Handles the backward propagation, which is essential for training neural networks using gradient-based optimization algorithms.\n",
    "    - **Parameters**:\n",
    "        - `downstream`: Represents the gradient of the loss concerning the output of this layer.\n",
    "    - **Behavior**: Computes the gradient concerning the inputs and weights of this layer by considering both the gradient of the activation function and the linear transformation.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: The utility of this layer comes into full effect when combined with non-linear activation functions. This combination allows neural networks to capture and model more complex and nuanced relationships in the data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ######\n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "\n",
    "class HiddenLayer(LinearLayer):\n",
    "    \"\"\"\n",
    "    Represents a hidden layer in a neural network. Inherits from the LinearLayer class\n",
    "    and adds activation functionality.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    forward(): Performs the forward pass, including linear transformation and activation.\n",
    "    backward(downstream): Performs the backward pass, including both activation and linear gradients.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dimension, output_dimension) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the HiddenLayer.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dimension: The number of input features.\n",
    "        output_dimension: The number of output features.\n",
    "        \"\"\"\n",
    "        super().__init__(input_dimension, output_dimension)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the forward pass by first conducting the linear transformation and then the activation.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        _out: The linearly transformed data.\n",
    "        \"\"\"\n",
    "        _out = super().forward()\n",
    "        return _out\n",
    "\n",
    "    def backward(self, downstream):\n",
    "        \"\"\"\n",
    "        Performs the backward pass by propagating the gradient through the activation function\n",
    "        and then through the linear transformation.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        downstream: The gradient of the loss with respect to the output of this layer.\n",
    "        \"\"\"\n",
    "        super().backward(downstream=downstream)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `LinearActivation` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "The `LinearActivation` class implements the linear activation function. This is essentially the identity function: for any input `x`, the output will be `x`. This activation is typically used in the output layer for regression tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods\n",
    "\n",
    "#### `forward(input_array: np.ndarray) -> np.ndarray`\n",
    "- **Purpose**: Computes the forward pass for linear activation.\n",
    "- **Parameters**: `input_array`: Input data or activations from the previous layer.\n",
    "- **Returns**: The same as `input_array`, because the linear activation doesn't change its input.\n",
    "- **Description**: For the linear activation, the output is the same as the input.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(downstream: np.ndarray, input_array=None) -> np.ndarray`\n",
    "- **Purpose**: Computes the backward pass (gradient) for linear activation.\n",
    "- **Parameters**: \n",
    "  - `downstream`: The gradient of the loss function with respect to the output of the linear activation.\n",
    "- **Returns**: Gradient of the loss with respect to the input of the linear activation.\n",
    "- **Description**: The derivative of the linear function is 1, so this operation multiplies it by the downstream gradient. However, since the derivative is 1, this operation doesn't change the downstream gradient.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ######\n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "class LinearActivation:\n",
    "    \"\"\"\n",
    "    Implements the linear activation function.\n",
    "\n",
    "    This activation is essentially the identity function. For any input 'x', the output will be 'x'.\n",
    "    It's commonly used in the output layer for regression tasks.\n",
    "\n",
    "    Methods:\n",
    "        forward(): Computes the forward pass by simply returning the input.\n",
    "        backward(): Computes the backward pass (derivative) which is 1 for the linear activation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(input_array):\n",
    "        \"\"\"\n",
    "        Computes the forward pass for linear activation.\n",
    "\n",
    "        Parameters:\n",
    "            input_array (np.ndarray): Input data or activations from the previous layer.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Same as input_array, as the linear activation doesn't change its input.\n",
    "        \"\"\"\n",
    "        # For linear activation, the output is same as input\n",
    "        return input_array\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(downstream, input_array=None):\n",
    "        \"\"\"\n",
    "        Computes the backward pass (gradient) for linear activation.\n",
    "\n",
    "        Parameters:\n",
    "            downstream_gradient (np.ndarray): The gradient of the loss function with respect to the output of the linear activation.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the input of the linear activation.\n",
    "        \"\"\"\n",
    "        # The derivative of the linear function is 1, so we just multiply it by the downstream gradient\n",
    "        # However, since the derivative is 1, this operation doesn't change the downstream gradient.\n",
    "        return 1 * downstream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `SigmoidActivation` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "The `SigmoidActivation` class implements the Sigmoid activation function. The sigmoid function is defined as: \n",
    "$$\\ f(z) = \\frac{1}{1 + e^{-z}} \\ $$\n",
    "where:\n",
    "- \\( z \\) is the input\n",
    "- \\( e \\) is the base of natural logarithms (approximately equal to 2.71828)\n",
    "\n",
    "### Attributes:\n",
    "- `input_layer`: The layer that provides input to this activation function.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `forward(input_array: np.ndarray) -> np.ndarray`\n",
    "- **Purpose**: Applies the Sigmoid activation function to the output of the input layer.\n",
    "- **Parameters**: `input_array`: Array of inputs to be passed through the sigmoid activation function.\n",
    "- **Returns**: The output after applying the sigmoid activation.\n",
    "- **Description**: \n",
    "  - Computes the negative exponential for every element in `input_array` using `np.exp()`.\n",
    "  - Calculates the denominator by adding 1 to every element of the previously computed negative exponential.\n",
    "  - Divides 1 by the computed denominator to get the sigmoid value.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(downstream: np.ndarray, input_array: np.ndarray) -> np.ndarray`\n",
    "- **Purpose**: Computes the gradient of the loss with respect to the input, and passes this back to the previous layers.\n",
    "- **Parameters**: \n",
    "  - `downstream`: The gradient of the loss with respect to the output of the sigmoid function.\n",
    "  - `input_array`: The original input to the sigmoid function.\n",
    "- **Returns**: Gradient of the loss with respect to the input of the sigmoid function.\n",
    "- **Description**: \n",
    "  - Uses the sigmoid formula from the forward function to get the sigmoid value.\n",
    "  - Computes the derivative of the sigmoid function using the formula: \n",
    "  $$ \\ ( f'(z) = f(z) \\times (1 - f(z)) \\ ) $$\n",
    "  - Calculates the gradient with respect to the input.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------#\n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "class SigmoidActivation:\n",
    "    \"\"\"\n",
    "    Implements the Sigmoid activation function.\n",
    "\n",
    "    The sigmoid function is defined as: f(z) = 1 / (1 + e^{-z})\n",
    "\n",
    "\n",
    "    Attributes:\n",
    "        input_layer: The layer that provides the input to this activation function.\n",
    "\n",
    "    Methods:\n",
    "        forward(input_array): Applies the Sigmoid activation function to the output of the input layer.\n",
    "        backward(downstream, input_array): Computes the gradient of the loss with respect to the input, which is then passed back to the previous layers.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(input_array):\n",
    "        \"\"\"\n",
    "        Compute the sigmoid activation for each element in the input_array.\n",
    "\n",
    "        The sigmoid activation function is defined as:\n",
    "        f(z) = 1 / (1 + e^(-z))\n",
    "\n",
    "        where:\n",
    "        - z is the input\n",
    "        - e is the base of natural logarithms (approximately equal to 2.71828)\n",
    "\n",
    "        Parameters:\n",
    "        - input_array: Array of inputs to be passed through the sigmoid activation function.\n",
    "\n",
    "        Returns:\n",
    "        - sigmoid: Array of outputs after applying the sigmoid activation.\n",
    "        \"\"\"\n",
    "        # Apply the Sigmoid activation function to the input array\n",
    "        # TODO 1. Compute the Exponential Term: For every element in the input array, you'll compute the negative exponential.\n",
    "        # In numpy, the function to compute the exponential of each element of an array is np.exp(). Using this function,\n",
    "        # compute the negative exponential of the input array.\n",
    "        negative_exponential = np.exp(-input_array)\n",
    "        # TODO 2. Compute the Denominator: Add 1 to every element of the previously computed negative_exponential array.\n",
    "        denominator = 1 + negative_exponential\n",
    "        # TODO 3. Divide 1 with the previously computed demonimator.\n",
    "        sigmoid = 1 / denominator\n",
    "        return sigmoid\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(downstream, input_array):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the loss with respect to the input of the sigmoid function.\n",
    "\n",
    "        The derivative of the sigmoid function f(z) with respect to its input z is:\n",
    "        f'(z) = f(z) * (1 - f(z))\n",
    "\n",
    "        where:\n",
    "        f(z) is sigmoid\n",
    "\n",
    "        Parameters:\n",
    "        - downstream: The gradient of the loss with respect to the output of the sigmoid function.\n",
    "        - input_array: The original input to the sigmoid function.\n",
    "\n",
    "        Returns:\n",
    "        - input_grad: The gradient of the loss with respect to the input of the sigmoid function (the derivative of the sigmoid function).\n",
    "        \"\"\"\n",
    "        # Compute the gradient of the loss with respect to the input\n",
    "        # TODO Duplicate the sigmoid code from the forward function here\n",
    "        negative_exponential = np.exp(input_array)\n",
    "        # TODO 2. Compute the Denominator: Add 1 to every element of the previously computed negative_exponential array.\n",
    "        denominator = 1 + negative_exponential\n",
    "        # TODO 3. Divide 1 with the previously computed demonimator.\n",
    "        sigmoid = 1 / denominator\n",
    "        # TODO Compute the derivative of the sigmoid function\n",
    "        sigmoid_derivative = sigmoid * (1 - sigmoid)\n",
    "        # TODO Compute the gradient with respect to the input by multiplying the gradient of the loss with respect to the output of sigmoid\n",
    "        # (downstream) by the derivative of the sigmoid (sigmoid_derivative) you computed above\n",
    "        input_grad = sigmoid_derivative * downstream\n",
    "        return input_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `ReLUActivation` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "The `ReLUActivation` class implements the Rectified Linear Unit (ReLU) activation function. The ReLU function is defined as: \n",
    "$$ \\ f(z) = \\max(0,z) \\ $$\n",
    "\n",
    "### Attributes:\n",
    "- **input_layer**: The layer that provides input to this activation function.\n",
    "- **input_dimension**: The shape of the output from the `input_layer`.\n",
    "- **output_dimension**: The shape of the output of this layer. For ReLU, this is the same as `input_dimension`.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `forward(input_array: np.ndarray) -> np.ndarray`\n",
    "- **Purpose**: Applies the ReLU activation function to the output of the `input_layer`.\n",
    "- **Parameters**: \n",
    "  - `input_array`: Array of inputs to be passed through the ReLU activation function.\n",
    "- **Returns**: The output after applying the ReLU activation.\n",
    "- **Description**: \n",
    "  - Uses the numpy function `np.maximum()` to return the element-wise maximum values from two arrays.\n",
    "  - For the ReLU activation, this means \\( f(x) = \\max(0, x) \\) where \\( x \\) is `input_array`.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(downstream: np.ndarray, input_array: np.ndarray = None) -> np.ndarray`\n",
    "- **Purpose**: Computes the gradient of the loss with respect to the input, and passes this back to the previous layers.\n",
    "- **Parameters**: \n",
    "  - `downstream`: The gradient of the loss with respect to the output of the ReLU function.\n",
    "  - `input_array`: The original input to the ReLU function. This is optional.\n",
    "- **Returns**: Gradient of the loss with respect to the input of the ReLU function.\n",
    "- **Description**: \n",
    "  - Computes the input gradient by multiplying the `downstream` gradient with an indicator array where values in the `input_array` greater than 0 are assigned a value of 1, and 0 otherwise.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------#\n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "\n",
    "class ReLUActivation:\n",
    "    \"\"\"\n",
    "    Implements the Rectified Linear Unit (ReLU) activation function.\n",
    "\n",
    "    The relu function is defined as: f(z) = max(0,z)\n",
    "\n",
    "    Attributes:\n",
    "        input_layer: The layer that feeds input into this activation function.\n",
    "        input_dimension: The shape of the output from the input_layer.\n",
    "        output_dimension: The shape of the output of this layer, which\n",
    "        is the same as the input_dimension for ReLU.\n",
    "\n",
    "    Methods:\n",
    "        forward(): Applies the ReLU activation function to the output\n",
    "        of the input layer.\n",
    "        backward(downstream): Computes the gradient of the loss with\n",
    "        respect to the input, to be passed back to the previous layers.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(input_array):\n",
    "        # TODO Use the numpy function np.maximum(). This function returns element-wise maximum values from two arrays.\n",
    "        # For the ReLU activation, the function is defined as f(x) = max(0, x) where x is your input_array\n",
    "        output_array = np.maximum(0, input_array)\n",
    "        return output_array\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(downstream, input_array=None):\n",
    "        input_grad = downstream * (input_array > 0)\n",
    "        return input_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `BiasLayer` Class\n",
    "\n",
    "This class represents a `BiasLayer` in a neural network. The layer's main function is to add a bias term to the output of the preceding input layer. For every feature dimension of the input, a bias term is added.\n",
    "\n",
    "### Attributes:\n",
    "\n",
    "- **input_layer**: The preceding layer in the neural network. \n",
    "- **output_dimension**: The shape of the output from this layer, which is the same as the `input_layer`'s output dimension.\n",
    "- **W**: Bias matrix, initialized with random values, to represent bias terms added to the input.\n",
    "- **activation**: Activation function object (if provided). It can be either `SigmoidActivation`, `ReLUActivation`, or None.\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `__init__(self, input_layer, activation=None) -> None`\n",
    "\n",
    "- **Description**: Initializes the `BiasLayer`.\n",
    "- **Parameters**:\n",
    "  - `input_layer`: The preceding layer in the neural network.\n",
    "  - `activation`: The type of activation function to use, if any.\n",
    "- **TODOs**:\n",
    "  - Declare and initialize the bias matrix `self.W` with the shape `(1, num_input_features)`.\n",
    "  - Instantiate the appropriate activation function based on the provided `activation` parameter.\n",
    "\n",
    "---\n",
    "\n",
    "#### `forward() -> np.ndarray`\n",
    "\n",
    "- **Description**: Performs the forward pass, adding the bias terms and then applying the activation function (if defined).\n",
    "- **Returns**: The activated output if an activation is defined, otherwise the output after just adding the bias.\n",
    "- **TODOs**:\n",
    "  - Fetch the output of the preceding layer and store it in `self.input_array`.\n",
    "  - Add the bias term (`self.W`) to each feature dimension of `self.input_array`.\n",
    "  - If an activation function is defined, apply it to the `self.output_array`.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(downstream: np.ndarray)`\n",
    "\n",
    "- **Description**: Computes the backward pass, propagating the gradient backwards through the activation (if defined) and then the bias addition.\n",
    "- **Parameters**:\n",
    "  - `downstream`: The gradient of the loss function with respect to the output of this layer.\n",
    "- **TODOs**:\n",
    "  - If an activation function is defined, compute the gradient of the loss with respect to the activated output, then propagate this gradient backward. If no activation is defined, propagate the downstream gradient directly backward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------#\n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "class BiasLayer:\n",
    "    \"\"\"\n",
    "    This layer adds a bias term to the output of the input layer.\n",
    "    Each feature dimension of the input gets a bias term.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_layer, activation=None) -> None:\n",
    "        self.input_layer = input_layer\n",
    "        num_data, num_input_features = input_layer.output_dimension\n",
    "        self.output_dimension = input_layer.output_dimension\n",
    "        # TODO: Declare the weight matrix (bias term) for the layer.\n",
    "        # Initialize a bias matrix `self.W` with the shape (1, num_input_features) using the `np.random.randn` function.\n",
    "        # This matrix will represent the bias terms added to the input.\n",
    "        self.W = np.random.randn(1, num_input_features)\n",
    "\n",
    "        if activation == 'Sigmoid':\n",
    "            self.activation = SigmoidActivation()\n",
    "\n",
    "        elif activation == 'ReLU':\n",
    "            self.activation = ReLUActivation()\n",
    "\n",
    "        else:\n",
    "            self.activation = LinearActivation()\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the bias layer.\n",
    "\n",
    "        Returns:\n",
    "        The output array after adding the bias terms.\n",
    "        \"\"\"\n",
    "        # TODO: Fetch the output of the preceding layer and store it in `self.input_array`.\n",
    "        # You can achieve this by calling the `forward` method of `self.input_layer`.\n",
    "        self.input_array = self.input_layer.forward()\n",
    "\n",
    "        # TODO: Add the bias term to the fetched input.\n",
    "        # Use the bias matrix `self.W` to add the bias term to each feature dimension of `self.input_array` and store the result in `self.output_array`.\n",
    "        self.output_array = self.input_array + self.W\n",
    "\n",
    "        # TODO: If an activation function is defined, apply it to the `self.output_array`.\n",
    "        # Call the `forward` method of the activation function and store the result in `self.activated_output`.\n",
    "        # Return the activated output. If no activation is defined, return the `self.output_array`.\n",
    "        if self.activation != None:\n",
    "            self.activated_output = self.activation.forward(self.output_array)\n",
    "            return self.activated_output\n",
    "\n",
    "        else:\n",
    "            return self.output_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def backward(self, downstream):\n",
    "        \"\"\"\n",
    "        Perform the backward pass.\n",
    "\n",
    "        Parameters:\n",
    "        - downstream: The gradient of the loss function with respect to the output of this layer.\n",
    "        \"\"\"\n",
    "        self.G = downstream\n",
    "\n",
    "        # TODO: If an activation function is defined, compute the gradient of the loss with respect to the activated output.\n",
    "        # Call the `backward` method of the activation function using the downstream gradient and the `self.activated_output` as arguments.\n",
    "        # Store the result in `activation_grad`.\n",
    "        # Then, compute the gradient of the loss with respect to the input of this layer by calling the `backward` method of `self.input_layer` with `activation_grad` as the argument.\n",
    "        # If no activation function is defined, directly call the `backward` method of `self.input_layer` using the downstream gradient as its argument.\n",
    "        if self.activation != None:\n",
    "            activation_grad = self.activation.backward(downstream, self.activated_output)\n",
    "            self.input_layer.backward(activation_grad)\n",
    "        else:\n",
    "            self.input_layer.backward(downstream)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions and Base Classes for Neural Network Implementations\n",
    "\n",
    "---\n",
    "\n",
    "### `is_modules_with_parameters` Function\n",
    "\n",
    "This function checks if the provided value is an instance of either `LinearLayer` or `BiasLayer`.\n",
    "\n",
    "#### Parameters:\n",
    "- `value` (object): The object to check.\n",
    "\n",
    "#### Returns:\n",
    "- `bool`: True if value is an instance of `LinearLayer` or `BiasLayer`, False otherwise.\n",
    "\n",
    "### `ModuleList` Class\n",
    "This class is an implementation of a mutable sequence to handle modules in a neural network.\n",
    "\n",
    "Methods:\n",
    "\n",
    "- `__getitem__`: Retrieve the i-th module.\n",
    "- `__setitem__`: Set the i-th module to v.\n",
    "- `__delitem__`: Delete the i-th module.\n",
    "- `__len__`: Return the number of modules.\n",
    "- `insert`: Insert module v at position i.\n",
    "- `get_modules_with_parameters`: Get modules that have parameters.\n",
    "\n",
    "### `BaseNetwork` Class\n",
    "This class serves as the base for neural network implementations.\n",
    "\n",
    "Methods:\n",
    "\n",
    "- `set_output_layer`: Set the output layer.\n",
    "- `get_output_layer`: Retrieve the output layer.\n",
    "- `__setattr__`: Overridden method to handle the setting of attributes, especially for modules with parameters.\n",
    "- `get_modules_with_parameters`: Get all modules that have parameters.\n",
    "- `forward`: Forward pass through the network.\n",
    "- `backward`: Backward pass through the network.\n",
    "- `state_dict`: Return the parameters of the modules.\n",
    "- `load_state_dict`: Load the parameters into the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ######\n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "def is_modules_with_parameters(value):\n",
    "    \"\"\"\n",
    "    Checks if the provided value is an instance of either LinearLayer or BiasLayer.\n",
    "\n",
    "    Parameters:\n",
    "    - value (object): The object to check.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if value is an instance of LinearLayer or BiasLayer, False otherwise.\n",
    "    \"\"\"\n",
    "    return isinstance(value, LinearLayer) or isinstance(value, BiasLayer)\n",
    "\n",
    "\n",
    "class ModuleList(collections.abc.MutableSequence):\n",
    "    \"\"\"\n",
    "    An implementation of a mutable sequence to handle modules in a neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.list = list()\n",
    "        self.list.extend(list(args))\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"Retrieve the i-th module.\"\"\"\n",
    "        return self.list[i]\n",
    "\n",
    "    def __setitem__(self, i, v):\n",
    "        \"\"\"Set the i-th module to v.\"\"\"\n",
    "        self.list[i] = v\n",
    "\n",
    "    def __delitem__(self, i):\n",
    "        \"\"\"Delete the i-th module.\"\"\"\n",
    "        del self.list[i]\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of modules.\"\"\"\n",
    "        return len(self.list)\n",
    "\n",
    "    def insert(self, i, v):\n",
    "        \"\"\"Insert module v at position i.\"\"\"\n",
    "        self.list.insert(i, v)\n",
    "        pass\n",
    "\n",
    "    def get_modules_with_parameters(self):\n",
    "        \"\"\"\n",
    "        Get modules that have parameters.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of modules with parameters.\n",
    "        \"\"\"\n",
    "        modules_with_parameters_list = []\n",
    "        for mod in self.list:\n",
    "            print(f\"Checking module: {mod}\")\n",
    "            if is_modules_with_parameters(mod):\n",
    "                print(f\"Adding module: {mod}\")\n",
    "                modules_with_parameters_list.append(mod)\n",
    "        print(f\"Final list of modules with parameters: {modules_with_parameters_list}\")\n",
    "        return modules_with_parameters_list\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class BaseNetwork:\n",
    "    \"\"\"\n",
    "    Base class for neural network implementations.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize BaseNetwork with an output layer and modules with parameters.\n",
    "        \"\"\"\n",
    "        super().__setattr__(\"initialized\", True)\n",
    "        super().__setattr__(\"modules_with_parameters\", [])\n",
    "        super().__setattr__(\"output_layer\", None)\n",
    "\n",
    "    def set_output_layer(self, layer):\n",
    "        \"\"\"Set the output layer.\"\"\"\n",
    "        super().__setattr__(\"output_layer\", layer)\n",
    "        pass\n",
    "\n",
    "    def get_output_layer(self):\n",
    "        \"\"\"Retrieve the output layer.\"\"\"\n",
    "        return self.output_layer\n",
    "\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        \"\"\"\n",
    "        Overridden method to handle the setting of attributes, especially for modules with parameters.\n",
    "        \"\"\"\n",
    "        print(f\"__setattr__ called with name: {name} and value: {value} type: {type(value)}\")\n",
    "\n",
    "        if not hasattr(self, \"initialized\") or (not self.initialized):\n",
    "            print(\"Initialization condition failed.\")\n",
    "            raise RuntimeError(\"You must call super().__init__() before assigning any layer in __init__().\")\n",
    "        print(\"Initialization condition passed.\")\n",
    "        if is_modules_with_parameters(value) or isinstance(value, ModuleList):\n",
    "            print(\"Module with parameters identified.\")\n",
    "            self.modules_with_parameters.append(value)\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "\n",
    "    def get_modules_with_parameters(self):\n",
    "        \"\"\"\n",
    "        Get all modules that have parameters.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of modules with parameters.\n",
    "        \"\"\"\n",
    "        modules_with_parameters_list = []\n",
    "        for mod in self.modules_with_parameters:\n",
    "            if isinstance(mod, ModuleList):\n",
    "                modules_with_parameters_list.extend(mod.get_modules_with_parameters())\n",
    "                pass\n",
    "            else:\n",
    "                modules_with_parameters_list.append(mod)\n",
    "                pass\n",
    "            pass\n",
    "        return modules_with_parameters_list\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        return self.output_layer.forward()\n",
    "\n",
    "    def backward(self, input_grad):\n",
    "        \"\"\"Backward pass through the network.\"\"\"\n",
    "        self.output_layer.backward(input_grad)\n",
    "        pass\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"\n",
    "        Return the parameters of the modules.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of parameters for all modules.\n",
    "        \"\"\"\n",
    "        all_params = []\n",
    "        for m in self.get_modules_with_parameters():\n",
    "            all_params.append(m.W)\n",
    "            pass\n",
    "        return all_params\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        \"\"\"\n",
    "        Load the parameters into the modules.\n",
    "\n",
    "        Parameters:\n",
    "        - state_dict (list): A list of parameters for the modules.\n",
    "        \"\"\"\n",
    "        assert len(state_dict) == len(self.get_modules_with_parameters())\n",
    "        for m, lw in zip(self.get_modules_with_parameters(), state_dict):\n",
    "            m.W = lw\n",
    "            pass\n",
    "        pass\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `SquareLoss` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `SquareLoss` class implements the square loss function, which is commonly used in regression tasks. It calculates the average squared difference between predicted values and ground truth labels.\n",
    "\n",
    "---\n",
    "\n",
    "### Attributes:\n",
    "- **input_layer**: The preceding layer of the neural network.\n",
    "- **labels**: Ground truth labels.\n",
    "- **num_data**: Number of data samples.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `__init__(self, input_dimension, labels=None)`\n",
    "- **Purpose**: Initializes an instance of the `SquareLoss` class.\n",
    "- **Parameters**: \n",
    "  - **input_dimension**: Dimensionality of the input data.\n",
    "  - **labels (default: None)**: Ground truth labels, if available.\n",
    "- **Description**: \n",
    "  - Sets up the `input_layer` based on the provided dimension.\n",
    "  - Initializes `labels` if they're provided and reshapes them if necessary.\n",
    "\n",
    "---\n",
    "\n",
    "#### `set_labels(self, labels)`\n",
    "- **Purpose**: Sets the labels for the loss calculation.\n",
    "- **Parameters**: \n",
    "  - **labels**: The ground truth labels.\n",
    "- **Description**: \n",
    "  - Updates the `labels` attribute.\n",
    "\n",
    "---\n",
    "\n",
    "#### `forward(self)`\n",
    "- **Purpose**: Computes the forward pass, calculating the square loss.\n",
    "- **Returns**: The computed square loss value.\n",
    "- **Description**: \n",
    "  - Calculates the square loss value using the formula \\( \\frac{1}{2M} || X - Y ||^2 \\), where \\( M \\) is the number of data samples, \\( X \\) is the predicted values, and \\( Y \\) is the ground truth labels.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(self)`\n",
    "- **Purpose**: Computes the backward pass, calculating the gradient of the loss.\n",
    "- **Description**: \n",
    "  - Calculates the gradient of the squared loss with respect to the output of the preceding layer. This gradient will be used by preceding layers during the backward propagation step.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: Several `TODO` comments are present in the code, suggesting areas for implementation or attention. Ensure you review and complete these as necessary.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------#\n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "class SquareLoss:\n",
    "\n",
    "    \"\"\"\n",
    "    Implements the square loss function, commonly used in regression tasks.\n",
    "\n",
    "    The square loss is defined as: \\( \\frac{1}{2M} || X - Y ||^2 \\)\n",
    "\n",
    "    Attributes:\n",
    "        input_layer: The preceding layer of the neural network.\n",
    "        labels: Ground truth labels.\n",
    "        num_data: Number of data samples.\n",
    "\n",
    "    Methods:\n",
    "        set_labels(labels): Method to set the labels.\n",
    "        forward(): Computes the forward pass, calculating the square loss.\n",
    "        backward(): Computes the backward pass, calculating the gradient of the loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_layer, labels=None) -> None:\n",
    "        self.input_layer = input_layer\n",
    "\n",
    "        self.labels = labels\n",
    "\n",
    "        # Reshape the labels if they are 1D.\n",
    "        if len(self.labels.shape) == 1:\n",
    "            self.labels = self.labels.reshape(-1, 1)\n",
    "\n",
    "    def set_labels(self, labels):\n",
    "        self.labels = labels\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Loss value is (1/2M) || X-Y ||^2\"\"\"\n",
    "        self.in_array = self.input_layer.forward()\n",
    "        # Determine the number of data samples.\n",
    "        self.num_data = self.in_array.shape[0]\n",
    "\n",
    "        # Implement the calculation of the squared loss:\n",
    "        # TODO: Subtract 'self.labels' from 'self.in_array' to get the element-wise difference between\n",
    "        # the predicted values and the ground truth labels.\n",
    "        difference = self.in_array - self.labels\n",
    "        # TODO: Use 'np.linalg.norm()' to compute the Euclidean norm (or L2 norm) of the difference.\n",
    "        # This function returns the Frobenius norm when used on matrices, which effectively\n",
    "        # calculates the root of the sum of the squared differences across all elements.\n",
    "        # Square the resulting norm to get the sum of squared differences.\n",
    "        euclidean_norm = np.linalg.norm(difference)\n",
    "        # TODO: Multiply the sum of squared differences in `euclidean_norm` by the results of dividing 0.5 by 'self.num_data'\n",
    "        # to compute the mean of the squared differences, which represents the average loss\n",
    "        # across all data samples.\n",
    "        # Store this result in 'self.out_array'.\n",
    "        self.out_array = euclidean_norm * (0.5 / self.num_data)\n",
    "\n",
    "        return self.out_array\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Gradient is (1/M) (X-Y), where M is the number of training samples\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Compute the element-wise difference between the predicted values (in 'self.in_array')\n",
    "        # and the actual ground truth values (in 'self.labels'). This difference represents the error\n",
    "        # for each data sample across all features.\n",
    "        # To compute the gradient of the squared loss with respect to each prediction, you need to\n",
    "        # divide each element of this difference by 'self.num_data'. This operation effectively scales\n",
    "        # down the error by the total number of data samples, giving you the average error gradient\n",
    "        # across all data samples. The resulting matrix/array will be the gradient of the loss\n",
    "        # with respect to the outputs of the preceding layer in the neural network. This gradient\n",
    "        # , termed 'self.pass_back', will be used by preceding layers to adjust their parameters\n",
    "        # during the backward propagation step.\n",
    "        self.pass_back = (1.0 / self.num_data) * (self.in_array - self.labels)\n",
    "\n",
    "        # Hand the gradient of loss with respect to inputs back to the previous layer.\n",
    "        self.input_layer.backward(self.pass_back)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Class: `AdamSolver`\n",
    "\n",
    "This class implements the Adam optimization algorithm. Adam is a method for efficient stochastic optimization that only requires first-order gradients concerning the objective to make updates to the parameters.\n",
    "\n",
    "The Adam update rule is given by:\n",
    "$$ \\ W = W - \\text{lr} \\times \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon} \\ $$\n",
    "\n",
    "Where:\n",
    "- $$ \\ \\hat{m} = \\frac{m}{1 - \\beta1^t} \\ $$\n",
    "- $$ \\ \\hat{v} = \\frac{v}{1 - \\beta2^t} \\ $$\n",
    "\n",
    "#### Parameters:\n",
    "- **`lr` (float):** Learning rate\n",
    "- **`modules` (List[LinearLayer]):** List of layers in the model (excluding the input layer)\n",
    "- **`beta1` (float, optional):** Exponential decay rate for the first moment estimate. Default is 0.9.\n",
    "- **`beta2` (float, optional):** Exponential decay rate for the second moment estimate. Default is 0.999.\n",
    "- **`epsilon` (float, optional):** A tiny constant to prevent division by zero. Default is 1e-8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------#\n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "\n",
    "class AdamSolver:\n",
    "    \"\"\"\n",
    "    Implements the Adam optimization algorithm.\n",
    "\n",
    "    Adam combines the benefits of both AdaGrad and RMSProp. It computes adaptive learning rates for each parameter.\n",
    "    In its equation, `m` and `v` are estimates of the first moment (the mean) and the second moment (the uncentered variance)\n",
    "    of the gradients respectively.\n",
    "\n",
    "    Update Rule:\n",
    "        m_t = beta1 * m_{t-1} + (1 - beta1) * g\n",
    "        v_t = beta2 * v_{t-1} + (1 - beta2) * g^2\n",
    "        m_hat = m_t / (1 - beta1^t)\n",
    "        v_hat = v_t / (1 - beta2^t)\n",
    "        w = w - lr * m_hat / (sqrt(v_hat) + epsilon)\n",
    "\n",
    "    Parameters:\n",
    "    - lr (float): Learning rate.\n",
    "    - modules (List[LinearLayer]): List of layers in the model (excluding the input layer).\n",
    "    - beta1 (float, optional): Exponential decay rate for first moment estimate. Default is 0.9.\n",
    "    - beta2 (float, optional): Exponential decay rate for second moment estimate. Default is 0.999.\n",
    "    - epsilon (float, optional): Small constant to prevent division by zero. Default is 1e-8.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate:float, modules: List[LinearLayer], beta1: float=0.9, beta2: float=0.999, epsilon: float=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.modules = modules\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "        for module in self.modules:\n",
    "            module.m = 0\n",
    "            module.v = 0\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Increment the time step `t`.\n",
    "        self.t += 1\n",
    "        for module in self.modules:\n",
    "            # g = 'module.G.mean(axis=0)' which computes the mean gradient across the batch.\n",
    "            g = module.G.mean(axis=0)\n",
    "\n",
    "            # TODO: Update biased first raw moment estimate (beta1 * m_{t-1} + (1 - beta1) * g)\n",
    "            module.m = self.beta1 * module.m + (1 - self.beta1) * g\n",
    "            # TODO: Update biased second raw moment estimate (beta2 * v_{t-1} + (1 - beta2) * g^2)\n",
    "            module.v = (self.beta2 * module.v + (1 - self.beta2) * g**2)\n",
    "            # TODO: Compute bias-corrected first moment estimate (module.m / (1 - beta1^t))\n",
    "            m_m_hat = (module.m / (1 - self.beta1 ** self.t))\n",
    "            # TODO: Compute bias-corrected second moment estimate (module.v / (1 - beta2^t))\n",
    "            m_v_hat = (module.v / (1 - self.beta2 ** self.t))\n",
    "\n",
    "            module.W -= self.learning_rate * m_m_hat / (np.sqrt(m_v_hat) + self.epsilon)\n",
    "            pass\n",
    "        pass\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `SGDSolver` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `SGDSolver` class implements the Stochastic Gradient Descent (SGD) optimization algorithm for neural networks.\n",
    "\n",
    "### Algorithm Description:\n",
    "\n",
    "SGD updates each parameter \\( W \\) based on the gradient \\( G \\) of the objective function with respect to that parameter. The formula for the parameter update is:\n",
    "\n",
    "\\[ W = W - \\text{lr} \\times \\text{mean}(G) \\]\n",
    "\n",
    "where \\(\\text{lr}\\) is the learning rate and \\(\\text{mean}(G)\\) is the mean of the gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### Attributes:\n",
    "- **learning_rate (float)**: The learning rate for the optimization.\n",
    "- **modules (List)**: List of layers in the model, excluding the input layer. All layers should have a parent class of `LinearLayer`.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `__init__(self, learning_rate: float, modules: List[LinearLayer])`\n",
    "- **Purpose**: Initializes an instance of the `SGDSolver` class.\n",
    "- **Parameters**: \n",
    "  - **learning_rate (float)**: Learning rate for optimization.\n",
    "  - **modules (List)**: List of layers in the model, excluding the input layer.\n",
    "- **Description**: \n",
    "  1. Initializes the `learning_rate` attribute with the given `learning_rate`.\n",
    "  2. Initializes the `modules` attribute with the given `modules` list. This list should contain all the layers (excluding the input layer) of the neural network model.\n",
    "\n",
    "---\n",
    "\n",
    "#### `step(self)`\n",
    "- **Purpose**: Performs a single optimization step, updating the parameters of all layers in 'modules'.\n",
    "- **Description**: \n",
    "  - This method updates the parameters of all layers in the 'modules' list according to the SGD update rule.\n",
    "  - For each layer, it computes the mean gradient of its weights. This can be done using `np.mean(gradient of the layer, axis=0)` where `layer.G` represents the gradient of the layer.\n",
    "  - It then uses the computed mean gradient to update the layer's weights using the SGD formula.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------#\n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "\n",
    "class SGDSolver:\n",
    "    \"\"\"\n",
    "    Implements the Stochastic Gradient Descent (SGD) optimization algorithm.\n",
    "\n",
    "    Algorithm Description:\n",
    "    SGD updates each parameter (W) based on the gradient (G) of the objective function\n",
    "    with respect to that parameter. The formula for the parameter update is:\n",
    "\n",
    "    W = W - lr * mean(G)\n",
    "\n",
    "    where lr is the learning rate and mean(G) is the mean of the gradients.\n",
    "\n",
    "    Parameters:\n",
    "    - learning_rate (float): Learning rate.\n",
    "    - modules (List): List of layers in the model, excluding the input layer.\n",
    "                       All layers should have a parent class of LinearLayer.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate:float, modules:List[LinearLayer]):\n",
    "        # TODO 1: Initialize the `learning_rate` attribute with the given 'learning_rate'.\n",
    "        self.learning_rate = learning_rate\n",
    "        # TODO 2: Initialize the `modules` attribute with the given 'modules' list. This list should contain all the layers (excluding the input layer) of the neural network model.\n",
    "        self.modules = modules\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Perform a single optimization step, updating the parameters of all layers in 'modules'.\n",
    "\n",
    "        Description:\n",
    "        ------------\n",
    "        The method should update the parameters of all layers in the 'modules' list according\n",
    "        to the SGD update rule.\n",
    "        \"\"\"\n",
    "\n",
    "        # Loop through each module (layer) present in the `self.modules` list.\n",
    "        # The formula to update weights is: W = W - lr * mean(G)\n",
    "        # Replace the weights of the module (module.W) with the updated weights.\n",
    "        for module in self.modules:\n",
    "            # TODO 4: For each module, compute the mean gradient of its weights. This can be done using `np.mean(gradient of the module, axis=0)` where `module.G` represents the gradient of the module.\n",
    "            mean_gradient = np.mean(module.G, axis=0)\n",
    "            # TODO 5: Use the computed mean gradient to update the module's weights using the SGD formula. Remember to scale the mean gradient with the learning rate `self.learning_rate` W = W - lr * mean gradient.\n",
    "            module.W = module.W - self.learning_rate * mean_gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `InputLayer` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `InputLayer` class represents the input layer of a neural network. The input layer essentially acts as a pass-through for the input data and does not perform any transformations.\n",
    "\n",
    "---\n",
    "\n",
    "### Attributes:\n",
    "- **W**: Identity matrix corresponding to the dimensions of the input data. This is included for compatibility with other layers but is not used.\n",
    "- **output_dimension**: The dimensions of the output (same as input in the case of `InputLayer`).\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `__init__(self, data_layer) -> None`\n",
    "- **Purpose**: Initializes an instance of the `InputLayer` class.\n",
    "- **Parameters**: \n",
    "  - **data_layer**: The layer that provides the input data to this layer.\n",
    "- **Description**: \n",
    "  - Initializes the output dimensions of this layer based on the provided `data_layer`.\n",
    "\n",
    "---\n",
    "\n",
    "#### `forward(self)`\n",
    "- **Purpose**: Passes the input data through the layer without altering it.\n",
    "- **Returns**: The input data without any modifications.\n",
    "- **Description**: \n",
    "  - Retrieves the input data from the associated `data_layer` and returns it as-is.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(self, downstream)`\n",
    "- **Purpose**: Placeholder function for the backward pass.\n",
    "- **Parameters**: \n",
    "  - **downstream**: The gradient of the loss with respect to the output of the input layer.\n",
    "- **Description**: \n",
    "  - This function is a placeholder and does not perform any actual backward pass computations in the input layer.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ######\n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "class InputLayer:\n",
    "    \"\"\"\n",
    "    Represents the input layer of a neural network. The input layer essentially acts as\n",
    "    a pass-through for the input data and does not perform any transformations.\n",
    "\n",
    "    Attributes:\n",
    "        W: Identity matrix corresponding to the dimensions of the input data.\n",
    "           This is included for compatibility with other layers but is not used.\n",
    "        output_dimension: The dimensions of the output (same as input in the case of InputLayer).\n",
    "\n",
    "    Methods:\n",
    "        forward(input_data): Passes the input data through the layer without altering it.\n",
    "        backward(downstream): Placeholder function; no actual backward pass computations\n",
    "                              are performed in the input layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_layer) -> None:\n",
    "        num_data, num_in_features = (data_layer.output_dimension)\n",
    "        self.output_dimension = np.array([num_data, num_in_features])\n",
    "        self.data_layer = data_layer\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.input_array = self.data_layer.forward()\n",
    "        return self.input_array\n",
    "\n",
    "    def backward(self, downstream):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `OutputLayer` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `OutputLayer` class represents the output layer in a neural network. It inherits from the `LinearLayer` class and allows the use of activation functions on the output.\n",
    "\n",
    "---\n",
    "\n",
    "### Attributes:\n",
    "- **activation**: The activation function to be used after the linear operation.\n",
    "- **activated_output**: The output after applying the activation function.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `__init__(self, input_layer, num_out_features)`\n",
    "- **Purpose**: Initializes an instance of the `OutputLayer` class.\n",
    "- **Parameters**: \n",
    "  - **input_layer**: The layer that provides input to this output layer.\n",
    "  - **num_out_features**: Number of features in the output.\n",
    "- **Description**: \n",
    "  - Calls the constructor of the superclass (`LinearLayer`) to initialize the layer.\n",
    "\n",
    "---\n",
    "\n",
    "#### `forward(self)`\n",
    "- **Purpose**: Performs the forward pass by applying the linear operation and the activation function (if any).\n",
    "- **Returns**: The output after the linear operation (and activation, if specified).\n",
    "- **Description**: \n",
    "  - Calls the forward method of the superclass (`LinearLayer`) and returns the result.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(self, downstream)`\n",
    "- **Purpose**: Computes the gradient of the loss with respect to the input of the output layer.\n",
    "- **Parameters**: \n",
    "  - **downstream**: The gradient of the loss with respect to the output of the output layer.\n",
    "- **Description**: \n",
    "  - Calls the backward method of the superclass (`LinearLayer`) to compute the gradients.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ######\n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "\n",
    "class OutputLayer(LinearLayer):\n",
    "    \"\"\"\n",
    "    Represents the output layer in a neural network, inheriting from the Linear class.\n",
    "    It allows the use of activation functions on the output.\n",
    "\n",
    "    Attributes:\n",
    "        activation: The activation function to be used after the linear operation.\n",
    "        activated_output: The output after applying the activation function.\n",
    "\n",
    "    Methods:\n",
    "        forward(): Performs the forward pass by applying the linear operation and activation function.\n",
    "        backward(dwnstrm): Performs the backward pass to compute the gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_layer, num_out_features):\n",
    "        super().__init__(input_layer, num_out_features)\n",
    "\n",
    "    def forward(self):\n",
    "        _out = super().forward()\n",
    "        return _out\n",
    "\n",
    "    def backward(self, downstream):\n",
    "        super().backward(downstream)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `Data` Class Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `Data` class is designed to store an input array of training data and pass it to the next layer. It's worth noting that while this class and the `InputLayer` could essentially perform the same function, they have been separated solely for learning purposes.\n",
    "\n",
    "---\n",
    "\n",
    "### Attributes:\n",
    "- **data**: The stored input array of training data.\n",
    "- **output_dimension**: The shape or dimensions of the stored data.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `__init__(self, data)`\n",
    "- **Purpose**: Initializes an instance of the `Data` class.\n",
    "- **Parameters**: \n",
    "  - **data**: The input array of training data.\n",
    "- **Description**: \n",
    "  - Stores the provided data and initializes the `output_dimension` attribute based on the shape of the data.\n",
    "\n",
    "---\n",
    "\n",
    "#### `set_data(self, data)`\n",
    "- **Purpose**: Updates the stored data.\n",
    "- **Parameters**: \n",
    "  - **data**: New input array of training data to be stored.\n",
    "- **Description**: \n",
    "  - Replaces the current stored data with the provided data.\n",
    "\n",
    "---\n",
    "\n",
    "#### `forward(self)`\n",
    "- **Purpose**: Passes the stored data forward.\n",
    "- **Returns**: The stored data.\n",
    "- **Description**: \n",
    "  - Simply returns the stored data without any modifications.\n",
    "\n",
    "---\n",
    "\n",
    "#### `backward(self, dwnstrm)`\n",
    "- **Purpose**: Placeholder function for the backward pass.\n",
    "- **Parameters**: \n",
    "  - **dwnstrm**: The gradient of the loss with respect to the output of the data.\n",
    "- **Description**: \n",
    "  - This function is a placeholder and does not perform any operations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ######\n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "\n",
    "class Data:\n",
    "    \"\"\"\n",
    "    Stores an input array of training data, and hands it to the next layer.\n",
    "    THIS AND THE INPUT LAYER COULD ESSENTIALLY PERFORM THE SAME FUCTION.\n",
    "    THEY HAVE BEEN SEPERATED SOLELY FOR LEARNING PURPOSES!!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.output_dimension = self.data.shape\n",
    "\n",
    "    def set_data(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def forward(self):\n",
    "        return self.data\n",
    "\n",
    "    def backward(self, dwnstrm):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `evaluate_model` Function Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `evaluate_model` function evaluates a model using various metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared, and potentially accuracy for classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **y_true (array-like)**: The ground truth target values.\n",
    "- **y_pred (array-like)**: The predicted values from the model.\n",
    "- **classification (bool, default: False)**: Specifies whether the task is classification. If `True`, accuracy will be computed; otherwise, only regression metrics will be provided.\n",
    "- **threshold (float, default: 0.5)**: The threshold for classifying outputs as positive for classification tasks. Outputs greater than this threshold will be classified as the positive class.\n",
    "\n",
    "---\n",
    "\n",
    "### Returns:\n",
    "\n",
    "- **dict**: A dictionary containing the computed metrics. This includes:\n",
    "  - `Mean Absolute Error`: The MAE between `y_true` and `y_pred`.\n",
    "  - `Mean Squared Error`: The MSE between `y_true` and `y_pred`.\n",
    "  - `R-squared`: The R-squared score between `y_true` and `y_pred`.\n",
    "  - `Accuracy` (optional): If `classification` is `True`, the accuracy of the predictions based on the provided threshold.\n",
    "\n",
    "---\n",
    "\n",
    "### Description:\n",
    "\n",
    "The function starts by computing the MAE, MSE, and R-squared between the true values (`y_true`) and the predicted values (`y_pred`). These values are stored in a dictionary named `metrics`.\n",
    "\n",
    "If the task is classification (i.e., `classification=True`), the function then converts probabilistic outputs to class labels based on the provided `threshold`. It computes the accuracy between the true labels and the predicted class labels and adds this accuracy value to the `metrics` dictionary.\n",
    "\n",
    "Finally, the function returns the `metrics` dictionary containing all computed values.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ######\n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "def evaluate_model(y_true, y_pred, classification=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate the model using MAE, MSE, R-squared, and accuracy metrics.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (array-like): The ground truth target values.\n",
    "        y_pred (array-like): The predicted values from the model.\n",
    "        classification (bool): Whether the task is classification or not. Default is False.\n",
    "        threshold (float): Threshold to classify as positive class for classification tasks. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the MAE, MSE, R-squared, and possibly accuracy values.\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'Mean Absolute Error': mae,\n",
    "        'Mean Squared Error': mse,\n",
    "        'R-squared': r2,\n",
    "    }\n",
    "\n",
    "    if classification:\n",
    "        # Convert probabilistic outputs to class labels based on threshold\n",
    "        y_pred_class = (y_pred > threshold).astype(int)\n",
    "        accuracy = accuracy_score(y_true, y_pred_class)\n",
    "        metrics['Accuracy'] = accuracy\n",
    "\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "This section provides the markdown documentation for the `Network` and `Trainer` classes, focusing on their structure, attributes, and methods. These classes are fundamental for defining and training a neural network.\n",
    "\n",
    "---\n",
    "\n",
    "## `Network` Class\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `Network` class inherits from the `BaseNetwork` and is responsible for defining the architecture of the neural network, including the input, hidden, and output layers.\n",
    "\n",
    "---\n",
    "\n",
    "### Attributes:\n",
    "\n",
    "- **MY_MODULE_LIST**: A module list that will store the various layers of the network.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `__init__(self, data_layer, hidden_layers, hidden_units)`\n",
    "- **Purpose**: Initializes an instance of the `Network` class.\n",
    "- **Description**: \n",
    "  - Initializes the base network.\n",
    "  - Constructs the neural network layers based on the provided parameters.\n",
    "  \n",
    "---\n",
    "\n",
    "## `Trainer` Class\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `Trainer` class is responsible for defining the network, setting it up, and performing training operations.\n",
    "\n",
    "---\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### `define_net(self, data_layer, parameters=None)`\n",
    "- **Purpose**: Defines the network architecture.\n",
    "- **Description**: \n",
    "  - Sets the hidden units and layers based on the provided parameters.\n",
    "  - Returns the defined network.\n",
    "  \n",
    "---\n",
    "\n",
    "#### `net_setup(self, train_data)`\n",
    "- **Purpose**: Sets up the network for training.\n",
    "- **Description**: \n",
    "  - Defines the data layer based on the provided training data.\n",
    "  - Defines the network, loss layer, and optimizer.\n",
    "  - Returns the data layer, network, loss layer, and optimizer.\n",
    "\n",
    "---\n",
    "\n",
    "#### `train_step(self)`\n",
    "- **Purpose**: Performs a single training step.\n",
    "- **Description**: \n",
    "  - Computes the loss, performs backward propagation, and updates the network parameters.\n",
    "  - Returns the computed loss.\n",
    "\n",
    "---\n",
    "\n",
    "#### `train(self, number_of_iterations)`\n",
    "- **Purpose**: Trains the network for a specified number of iterations.\n",
    "- **Description**: \n",
    "  - Iteratively performs training steps and stores the loss for each step.\n",
    "  - Returns a list of training losses.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------#\n",
    "#--------- THIS CELL NEEDS TO BE EDITED!! WE HAVE INCLUDED TODO COMMENT(S) ----------#\n",
    "#--------------------------- TO GUIDE YOUR IMPLEMENTATION ---------------------------#\n",
    "#------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "# TODO Experiment to find the munber of iterations\n",
    "Number_of_iterations = 300\n",
    "# TODO Experiment to find the right step_size/learning_rate for the network\n",
    "learning_rate = 0.001\n",
    "\n",
    "class Network(BaseNetwork):\n",
    "    def __init__(self, data_layer, hidden_layers, hidden_units):\n",
    "        super().__init__()\n",
    "        self.MY_MODULE_LIST = ModuleList()\n",
    "        self.MY_MODULE_LIST.append(InputLayer(data_layer))\n",
    "        for _ in range(hidden_layers):\n",
    "            # TODO Since each hidden layer is accompanied to a bias, use the for loop to add\n",
    "            # the number of hidden layers passed in and their corresponding bias layers.\n",
    "            # Don't forget to add an activation fuction for your biases.\n",
    "            # You can find the last layer in the list by using the list index or the __getitem__\n",
    "            # and passing it the index of the length of the list minus 1 (len(self.MY_MODULE_LIST) - 1)\n",
    "            self.MY_MODULE_LIST.append(HiddenLayer(input_dimension=self.MY_MODULE_LIST.__getitem__(len(self.MY_MODULE_LIST) - 1), output_dimension=random.choice(hidden_units)))\n",
    "            self.MY_MODULE_LIST.append(BiasLayer(input_layer=self.MY_MODULE_LIST.__getitem__(len(self.MY_MODULE_LIST) - 1), activation='ReLU'))\n",
    "\n",
    "        # TODO Now add the output layer to the MODULE LIST.\n",
    "        self.MY_MODULE_LIST.append(OutputLayer(input_layer=self.MY_MODULE_LIST.__getitem__(len(self.MY_MODULE_LIST) - 1), num_out_features=1))\n",
    "        # TODO set the output layer to the last layer in self.MY_MODULE_LIST.\n",
    "        self.set_output_layer(self.MY_MODULE_LIST.__getitem__(len(self.MY_MODULE_LIST) - 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def define_net(self, data_layer, parameters=None):\n",
    "        # TODO this network is a deep network and hense, has multiple hidden layers and multiple hidden units\n",
    "        # set the hidden units and layers to the values passed in parameters\n",
    "        hidden_units = parameters['hidden_units']\n",
    "        hidden_layers = parameters['hidden_layers']\n",
    "        network = Network(data_layer, hidden_layers=hidden_layers, hidden_units=hidden_units)\n",
    "        print(network)\n",
    "        return network\n",
    "\n",
    "    def net_setup(self, train_data):\n",
    "        features, labels = train_data\n",
    "        self.data_layer = Data(features)\n",
    "        # TODO pass in an Integer for hidden_layers and a list of integers to hidden_units\n",
    "        self.network = self.define_net(self.data_layer, parameters={'hidden_layers': 5, 'hidden_units': [50]})\n",
    "        # TODO Select the approraiate loss function and pass it the required parameters\n",
    "        self.loss_layer = SquareLoss(self.network.get_output_layer(), labels)\n",
    "        # TODO Select the approraiate optimizer and pass it the required parameters\n",
    "        self.optimizer = AdamSolver(learning_rate, self.network.get_modules_with_parameters())\n",
    "        return self.data_layer, self.network, self.loss_layer, self.optimizer\n",
    "\n",
    "    def train_step(self):\n",
    "        loss = self.loss_layer.forward()\n",
    "        self.loss_layer.backward()\n",
    "        self.optimizer.step()\n",
    "        # print(loss)\n",
    "        return loss\n",
    "\n",
    "    def train(self, number_of_iterations):\n",
    "        train_losses = []\n",
    "\n",
    "        # for _ in range(number_of_iterations):\n",
    "        #     train_losses.append(self.train_step())\n",
    "        for _ in tqdm(range(number_of_iterations), desc=\"Training\", leave=True):\n",
    "            train_losses.append(self.train_step())\n",
    "\n",
    "        return train_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `visualize_data` Function Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `visualize_data` function is designed to visualize the actual vs. predicted values of a model's output for each feature in the dataset. It creates a scatter plot for each feature, contrasting the actual target values with the predicted values.\n",
    "\n",
    "---\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **x_test**: The input test data.\n",
    "- **y_test**: The true target values.\n",
    "- **y_pred**: The predicted target values by the model.\n",
    "\n",
    "---\n",
    "\n",
    "### Description:\n",
    "\n",
    "1. **Setup**:\n",
    "    - The function begins by determining the number of features in `x_test`.\n",
    "    - A subplot is created for each feature, resulting in multiple scatter plots arranged vertically.\n",
    "\n",
    "2. **Visualization**:\n",
    "    - For each feature in `x_test`:\n",
    "        - Actual target values (`y_test`) are plotted against the feature values using circles.\n",
    "        - Predicted target values (`y_pred`) are plotted against the same feature values using crosses.\n",
    "        - Each subplot is labeled with appropriate x and y axis labels, a legend distinguishing 'Actual' from 'Predicted', and a title indicating which feature the plot corresponds to.\n",
    "\n",
    "3. **Display**:\n",
    "    - The plots are organized using `plt.tight_layout()` to ensure no overlap or congestion.\n",
    "    - All the scatter plots are displayed using `plt.show()`.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This function is particularly useful for visually assessing the performance of regression models on multi-feature datasets. By inspecting the scatter plots, one can gain insights into how well the model predictions align with the actual values for different features.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ######\n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "def visualize_data(x_test, y_test, y_pred):\n",
    "    # Number of features\n",
    "    num_features = x_test.shape[1]\n",
    "\n",
    "    # Create a subplot for each feature\n",
    "    fig, axes = plt.subplots(nrows=num_features, figsize=(8, 4 * num_features))\n",
    "\n",
    "    # For each feature, plot actual vs predicted\n",
    "    for i in range(num_features):\n",
    "        ax = axes[i]\n",
    "        ax.scatter(x_test[:, i], y_test, alpha=0.5, label='Actual')\n",
    "        ax.scatter(x_test[:, i], y_pred, alpha=0.5, label='Predicted', marker='x')\n",
    "        ax.set_xlabel(f'Feature_{i + 1}')\n",
    "        ax.set_ylabel('Target')\n",
    "        ax.legend()\n",
    "        ax.set_title(f'Scatter Plot for Feature_{i + 1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `main` Function Documentation\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `main` function is designed to set up a training environment using the `Trainer` class. It orchestrates the process of data loading, network setup, training, and evaluation. It also plots training losses over iterations and displays evaluation metrics. If in test mode, it returns specific objects necessary for automated testing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ######\n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "def main(test=False):\n",
    "    # setup the trainer\n",
    "    trainer = Trainer()\n",
    "\n",
    "    # DO NOT REMOVE THESE IF/ELSE\n",
    "    if not test:\n",
    "        # Your code goes here.\n",
    "        data = q2_b()\n",
    "        data_layer, network, loss_layer, optimizer = trainer.net_setup(data['train'])\n",
    "        loss = trainer.train(Number_of_iterations)\n",
    "        plt.plot(loss)\n",
    "        plt.ylabel('Loss of NN')\n",
    "        plt.xlabel('Number of Iterations')\n",
    "        plt.show()\n",
    "\n",
    "        # Now let's use the test data\n",
    "        x_test, y_test = data['test']\n",
    "        test_data_layer = Data(x_test)\n",
    "        network.input_layer = InputLayer(test_data_layer)\n",
    "        trainer.data_layer.set_data(network.input_layer)\n",
    "        trainer.network.MY_MODULE_LIST[1].input_layer = network.input_layer\n",
    "\n",
    "        # Get predictions for test data\n",
    "        y_pred = network.MY_MODULE_LIST[-1].forward()\n",
    "\n",
    "        metrics = evaluate_model(y_test, y_pred)\n",
    "        # Print the metrics for review\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "        visualize_data(x_test, y_test, y_pred)\n",
    "\n",
    "    else:\n",
    "        # DO NOT CHANGE THIS BRANCH! This branch is used for autograder.\n",
    "        out = {\n",
    "            'trainer': trainer\n",
    "        }\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 5) (20000, 5) (80000, 1) (20000, 1)\n",
      "[[3.4091685  3.088336   6.134263   8.280664   9.275371  ]\n",
      " [0.40502432 9.104359   6.3096724  7.2207837  5.689268  ]\n",
      " [3.7081108  2.8466923  7.9519215  7.858429   4.2896347 ]\n",
      " ...\n",
      " [7.81615    7.0501266  5.0537057  5.5682178  0.5605876 ]\n",
      " [0.50829166 3.0850742  9.097385   8.045393   2.9717112 ]\n",
      " [7.0318084  7.614908   6.338875   4.659026   2.2593691 ]]\n",
      "[[0.6996606 ]\n",
      " [0.9820171 ]\n",
      " [0.83718246]\n",
      " ...\n",
      " [0.6269969 ]\n",
      " [2.0155513 ]\n",
      " [1.4814262 ]]\n",
      "__setattr__ called with name: MY_MODULE_LIST and value: <__main__.ModuleList object at 0x0000020D3FA18490> type: <class '__main__.ModuleList'>\n",
      "Initialization condition passed.\n",
      "Module with parameters identified.\n",
      "<__main__.Network object at 0x0000020D3FA18EE0>\n",
      "Checking module: <__main__.InputLayer object at 0x0000020D3FA18610>\n",
      "Checking module: <__main__.HiddenLayer object at 0x0000020D3FA18700>\n",
      "Adding module: <__main__.HiddenLayer object at 0x0000020D3FA18700>\n",
      "Checking module: <__main__.BiasLayer object at 0x0000020D3FA18160>\n",
      "Adding module: <__main__.BiasLayer object at 0x0000020D3FA18160>\n",
      "Checking module: <__main__.HiddenLayer object at 0x0000020D3FA18430>\n",
      "Adding module: <__main__.HiddenLayer object at 0x0000020D3FA18430>\n",
      "Checking module: <__main__.BiasLayer object at 0x0000020D3FA18B80>\n",
      "Adding module: <__main__.BiasLayer object at 0x0000020D3FA18B80>\n",
      "Checking module: <__main__.HiddenLayer object at 0x0000020D3FA18BB0>\n",
      "Adding module: <__main__.HiddenLayer object at 0x0000020D3FA18BB0>\n",
      "Checking module: <__main__.BiasLayer object at 0x0000020D3FA18100>\n",
      "Adding module: <__main__.BiasLayer object at 0x0000020D3FA18100>\n",
      "Checking module: <__main__.HiddenLayer object at 0x0000020D3FA18B50>\n",
      "Adding module: <__main__.HiddenLayer object at 0x0000020D3FA18B50>\n",
      "Checking module: <__main__.BiasLayer object at 0x0000020D3FA185E0>\n",
      "Adding module: <__main__.BiasLayer object at 0x0000020D3FA185E0>\n",
      "Checking module: <__main__.HiddenLayer object at 0x0000020D3FA184F0>\n",
      "Adding module: <__main__.HiddenLayer object at 0x0000020D3FA184F0>\n",
      "Checking module: <__main__.BiasLayer object at 0x0000020D3FA18520>\n",
      "Adding module: <__main__.BiasLayer object at 0x0000020D3FA18520>\n",
      "Checking module: <__main__.OutputLayer object at 0x0000020D3FA18D60>\n",
      "Adding module: <__main__.OutputLayer object at 0x0000020D3FA18D60>\n",
      "Final list of modules with parameters: [<__main__.HiddenLayer object at 0x0000020D3FA18700>, <__main__.BiasLayer object at 0x0000020D3FA18160>, <__main__.HiddenLayer object at 0x0000020D3FA18430>, <__main__.BiasLayer object at 0x0000020D3FA18B80>, <__main__.HiddenLayer object at 0x0000020D3FA18BB0>, <__main__.BiasLayer object at 0x0000020D3FA18100>, <__main__.HiddenLayer object at 0x0000020D3FA18B50>, <__main__.BiasLayer object at 0x0000020D3FA185E0>, <__main__.HiddenLayer object at 0x0000020D3FA184F0>, <__main__.BiasLayer object at 0x0000020D3FA18520>, <__main__.OutputLayer object at 0x0000020D3FA18D60>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▋         | 19/300 [56:24<13:54:18, 178.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\masre\\Desktop\\School stuff\\CMSC421\\CMSC421-NeuralNetworkProject\\New_Code_Base\\q3_b.ipynb Cell 39\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m######################################################################################\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ######\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m######################################################################################\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     main()\n",
      "\u001b[1;32mc:\\Users\\masre\\Desktop\\School stuff\\CMSC421\\CMSC421-NeuralNetworkProject\\New_Code_Base\\q3_b.ipynb Cell 39\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m data \u001b[39m=\u001b[39m q2_b()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m data_layer, network, loss_layer, optimizer \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mnet_setup(data[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(Number_of_iterations)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mLoss of NN\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\masre\\Desktop\\School stuff\\CMSC421\\CMSC421-NeuralNetworkProject\\New_Code_Base\\q3_b.ipynb Cell 39\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# for _ in range(number_of_iterations):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m#     train_losses.append(self.train_step())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(number_of_iterations), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mreturn\u001b[39;00m train_losses\n",
      "\u001b[1;32mc:\\Users\\masre\\Desktop\\School stuff\\CMSC421\\CMSC421-NeuralNetworkProject\\New_Code_Base\\q3_b.ipynb Cell 39\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_layer\u001b[39m.\u001b[39mforward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_layer\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# print(loss)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "\u001b[1;32mc:\\Users\\masre\\Desktop\\School stuff\\CMSC421\\CMSC421-NeuralNetworkProject\\New_Code_Base\\q3_b.ipynb Cell 39\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodules:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39m# g = 'module.G.mean(axis=0)' which computes the mean gradient across the batch.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     g \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49mG\u001b[39m.\u001b[39;49mmean(axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39m# TODO: Update biased first raw moment estimate (beta1 * m_{t-1} + (1 - beta1) * g)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/masre/Desktop/School%20stuff/CMSC421/CMSC421-NeuralNetworkProject/New_Code_Base/q3_b.ipynb#X53sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     module\u001b[39m.\u001b[39mm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta1 \u001b[39m*\u001b[39m module\u001b[39m.\u001b[39mm \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta1) \u001b[39m*\u001b[39m g\n",
      "File \u001b[1;32mc:\\Users\\masre\\Desktop\\School stuff\\CMSC421\\CMSC421-NeuralNetworkProject\\New_Code_Base\\myenv\\lib\\site-packages\\numpy\\core\\_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    115\u001b[0m         dtype \u001b[39m=\u001b[39m mu\u001b[39m.\u001b[39mdtype(\u001b[39m'\u001b[39m\u001b[39mf4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m         is_float16_result \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m ret \u001b[39m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, mu\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    120\u001b[0m     \u001b[39mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################################################################################\n",
    "######  THIS CELL DOES NOT NEED TO BE EDITED!! FEEL FREE TO READ THROUGH IT TO  ######\n",
    "#############  UNDERSTAND HOW ITS BEING USED BUT IT CAN SAFELY BE IGNORED  ###########\n",
    "######################################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
